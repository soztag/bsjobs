--- 
title: "'Bullshit Jobs'"
author: 
  - name: "Marco Blank"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Maximilian Held"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Verena Kasztantowicz"
    affiliation: "Humboldt-Universität zu Berlin (HU)"
  - name: "Horan Lee"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Manuel Nicklich"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Sabine Pfeiffer"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Stefan Sauer"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
  - name: "Amelie Tihlarik"
    affiliation: "Friedrich-Alexander Universität Erlangen-Nürnberg (FAU)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: soztag_fgm.bib
editor_options: 
  chunk_output_type: console
---

<div class="jumbotron" style="color:white; background: linear-gradient( rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5) ), url(img/902px-Punishment_sisyph.jpg) no-repeat center center fixed; -webkit-background-size: cover; -moz-background-size: cover; -o-background-size: cover; background-size: cover;">
  <h2>Bullshit Jobs</h1>
  <p>Eine arbeitssoziologische Reflektion</p>
  <p>
    <span class="label label-info">
      #bsjobs
    </span>
  </p>
  <p><small><sub>
    Image Credit: [*Sisyphus*](https://en.wikipedia.org/wiki/File:Punishment_sisyph.jpg) (1548--1549) by Titian, Prado Museum, Madrid, Spain
  </sub></small></p>
</div>

```{r, child="README.rmd"}
```

---

```{r setup, cache = FALSE, include = FALSE}
source("setup.R")
```

# Pragmatische Moralkritik

<div class="alert alert-warning">
**Entwurf**
</div>

@GraeberBullshitJobsTheory2018 definiert "Bullshit Jobs" als *subjektives* Konzept, also ausgehend von der Selbsteinschätzung der sozialen Nützlichkeit der eigenen Arbeit.
Dies geht soweit, dass die Existenz dieser Tätigkeit moralisch abgelehnt wird.
Aus dieser Perspektive kann das Phänomen Bullshit Jobs auch als Moralkritik unserer Wirtschaftsweise verstanden werden.
Bullshit Jobs gelten dann nicht als a posteriori positiver Befund über die *Verbreitung* von vermeintlich "unnützer" oder "schädlicher" Arbeit, sondern --- wohlwollend gelesen --- ein axiomatischer Geltungsanspruch zu *guter* Arbeit.
Gute Arbeit wäre dann als anderen Menschen dienlich empfundene Arbeit.

Graeber reißt die Widersprüche dieser Definition --- *subjektiv* anderen *nützliche* Arbeit --- an, kann sie aber nicht stringent auflösen.
Zwischen den gleichermaßen tautologischen Definitionen der Neoklassik einerseits (Nutzen als Zahlungsbereitschaft) und des Marxismus andererseits (Gebrauchswert als situativ und qualitativ), kommt Graebers hemdsärmliche Operationalisierung aber als erfrischend pragmatischer Versuch daher.

*Wenn* die Kritik der Bullshit Jobs Menschen treffend erscheint, um über ihre und die Arbeit anderer zu reflektieren und eine moralische Unterscheidung zu treffen, dann kann sie *möglicherwiese* auch im Sinne eines "*ökologischen*" (engl. "ecological") Pragmatismus gelten [vgl. @Peirce-1903].
In James' prägnanter Formulierung haben Bullshit Jobs den "Barwert ihrer Wahrheit" [-@JamesPragmatismNewName1907: 200] eingelöst, wenn sie Menschen befähigen, *alternative* und *gangbare* wirtschaftliche Arrangements einzufordern.
Bullshit Jobs --- erneut wohlwollend gelesen --- könnten dann als charakteristisch kontingente, vorläufige und *erstrebende* Forderungen eines amerikanischen Pragmatismus verstanden werden [vgl. @Dewey-1916].
Im Unterschied zum Marxismus sind Bullshit Jobs nicht "entfremdend" [@Marx-1867-aa].
Das Konzept widerspricht einem *konstanten* menschlichen Gattungswesen [vgl. @HaugDoppelcharakterArbeit] und lehnt die Vorstellung eines überdauernden menschengerechten Arrangement von Arbeit ab.
Stattdessen werden Bullshit Jobs vor dem Hintergrund *konkreter*, gegenwärtiger technologischer und organisationeller Innovation als unnütz erkannt.
Ebenso sind Bullshit Jobs nicht schlicht eine (mutmaßlich durch staatliche Verzerrung verursachte) Abweichung von kompetitiven Märkten, oder definitorische Unmöglichkeiten (da nicht Pareto-verbessernd) einer katallaktischen Ordnung [@Hayek-1960], wie aus neoklassischer oder libertärer Sicht argumentiert werden könnte.
Sie bleiben trotz und unabhängig von ihrer gegenwärtigen ökonomischen Rationalität irrsinnig, da es gleichzeitig subjektiv nützlichere Tätigkeiten *gibt*, und das Preissystem damit offenkundig nicht seiner vermittelnden Aufgabe nachkommt.

Hier zeigt sich eine weitere mögliche pragmatische Qualität der Bullshit Jobs Kritik:
Graeber kritisiert existierende Jobs in Hinblick auf konkrete Technologien und Organisation die selbige ersetzen könnten, und *im Vergleich* zu anderer, existierender, *nützlicherer* Arbeit, von der mehr zu erledigen wäre, oder höher geschätzt und entlohnt werden wollen.
Pragmatisch gelesen steht Graebers Kritik damit entgegen etablierten Konzepten zur guten Gestaltung von Arbeitsbedingungen, und stellt auf den subjektiv erfahrene Inhalt eines Jobs ab.
Überrasschenderweise ähnelt Graeber damit unter anderem der Moraltheorie --- nicht der Ökonomie --- von @Smiththeorymoralsentiments1822.


## Kommentare auf dem Social-News-Aggregator *"reddit"*

Aus zweierlei Hinsicht ist es sinnvoll, öffentliche Reaktionen auf Graebers "Bullshit Jobs" näher zu untersuchen:

1. Zum einen erfordert Graeber subjektive Operationalisierung von Bullshit Jobs weitere Untermauerung.
     Graebers selbst vorgetragene Empirie ist überwiegend anektodisch, selbst-selektiert (durch E-Mail-Korrespondenz *an* den Autor) und sowohl qualitativ als auch quantitativ mangelhaft.
    Die These: Sollten tatsächlich viele Menschen ihre Arbeit als nutzlos empfinden, dann sollte sich dies in öffentlichen Reaktionen niederschlagen, jedenfalls wenn Anonymitität gewährleistet ist.
    Da auch die Definition von Bullshit Jobs lückenhaft ist, ist ebenso von Interesse *welche Qualitäten* Arbeit nutzlos erscheinen lassen.
    
2. Auch der ökologisch-pragmatische *Nutzen* sollte sich in öffentlichen Reaktionen niederschlagen.
     Zwar kann eine Moralkritik nicht a posteriori positiv überprüft werden, im pragmatischen Sinne sollte sie aber [wie in @GraeberBullshitJobsTheory2018] Menschen dienlich sein, dadurch alternative Arrangements *fordern zu können*.
     Es gilt insofern zu überprüfen, ob und in welcher Weise, die Kritik der Bullshit Jobs verfängt.

```{r import_via_praw, eval=FALSE, include=FALSE}
library(RedditExtractoR)
urls <- RedditExtractoR::reddit_urls(search_terms = "'bullshit jobs'", cn_threshold = 1) %>% 
  as_tibble() %>% 
  select(title, subreddit, URL) %>% 
  mutate(submission_id = map_chr(.x = stringr::str_split(string = urls$URL, pattern = "/"), 7))

library(reticulate)
os <- import(module = "os")  # just to access the os via python and figure out current wd
checkmate::assert_true(x = getwd() == os$getcwd()) 
praw <- import(module = "praw")
# this calls credentials stored in praw.ini, but gitignored here
# as per https://praw.readthedocs.io/en/latest/getting_started/configuration/environment_variables.html
py_objs <- py_run_file(file = "praw.py", convert = TRUE)

# loop over all urls
map2_dfr(
  .x = urls$submission_id,
  .y = urls$title, 
  .f = function(submission_id, submission_title) {
    py_objs$getAllForOneURL(submission_id) %>% 
      discard(.p = function(x) {
        # these are just "more comments" buttons
        inherits(x = x, what = "praw.models.reddit.more.MoreComments")
      }) %>% 
      map_dfr(.id = "index", .f = function(x, y) {
        tibble(
          submission_id = submission_id,
          id = x$id,
          body = x$body,
          score = x$score,
          # sometimes authors are NULL for some reason
          author = ifelse(test = is.null(x$author$name), yes = NA, no = x$author$name),
          submission_title = submission_title
        )
      })
  }
) %>% 
  readr::write_rds(path = "r_coms.rds")
```

Im Sinne einer methodischen Triangulation ergänzen wir Graebers anekdotische Empirie durch eine quantitative, "large N" Textanalyse von im Web veröffentlichen Kommentaren zu "Bullshit Jobs".

```{r}
r_coms <- readr::read_rds(path = "r_coms.rds")
```

Vor allem aus forschungspraktischen Gründen nutzen wir hierzu Einträge auf dem Social News Aggregator [reddit.com](http:://www.reddit.com):
Die Plattform bietet für Forschende großzügige Nutzungsbedingungen, und ist durch eine ausgereifte API mit entsprechendem Python Package gut erschlossen [@BoePythonRedditAPI2016].
So war es möglich in recht knapper Zeit **`r nrow(r_coms)`** Volltext-Kommentare von **`r length(unique(r_coms$author))`** unterschiedlichen Nutzernamen zu **`r length(unique(r_coms$submission_id))`** Einreichungen in einer Vollerhebung zu sammeln.

In der Interpretation der unten stehende Ergebnisse sollten einige Vor- und Nachteile der reddit-Daten berücksichtig werden.

**Vorteile**:

- Die auf reddit übliche Anonymität und Offenheit sollte die von Graeber befürchtete soziale Erwünschtheit der Antworten deutlich dämpfen.
  Tatsächlich finden sich in den Kommentaren einzelne offene Beschreibungen der eigenen Arbeit als "sinnlos".
- Im Unterschied zu Einträgen auf anderen sozialen Medien (etwa Twitter), sind die Einlassungen auf reddit länger und enthalten typischerweise umfangreiche Einschätzungen und Bewertungen.
  Reine "link posts" gibt es in den Kommentaren kaum.
- Alle Kommentare sind abgegeben auf Einreichungen ("Threads"), die explizit Bullshit Jobs im Sinne Graebers thematisieren, oft mit Nennung des Buches.


**Nachteile**:

- Zwar sind alle Einreichungen zum Thema Bullshit Jobs, allerdings sind die entsprechenden Links zumeist *nicht* das tatsächliche Buch, sondern zu Besprechungen, hier vor allem zu einem Artikel auf [wired.co.uk](https://www.wired.co.uk/article/bullshit-jobs-david-graeber-review) [@UpchurchForgetfearsautomation2018].
  Es muss daher davon ausgegangen werden, dass viele Kommentatoren bestenfalls den Artikel, nicht jedoch das Buch gelesen haben.
- Der Artikel behandelt zwar mehrere Aspekte des Buches, die Überschrift ("Forget fears of automation, your job is probably bullshit anyway") und Schlussfolgerung verengt die Thesen Buches jedoch auf technologischen Wandel und den Vorschlag eines bedingungslosen Grundeinkommens ("universal basic income", UBI).
  Viele der Kommentare beschäftigen sich dann auch eher mit diesen Themen, als einer breiteren Auseinandersetzung mit Bullshit Jobs und der eigenen Arbeitserfahrung.
- Wie auch Graebers Gesprächspartner sind die Kommentatoren auf reddit stark selbstselektiert.
  Vermutlich sind die Kommentatoren jünger, technik-affiner und anglo-amerikanischer als ein Bevölkerungsquerschnitt.
- Auf Besonderheiten der europäischen oder gar deutschen Situation können mit den Daten keine Rückschlusse gezogen werden.


### Analyse

<div class="alert alert-warning">
**Entwurf**
</div>

Zunächst wurden Kommentare in Zeilen, und dann mittels des [tidytext](https://www.tidytextmining.com) package in einzelne Wörter tokenisiert [@SilgeTextMining].
Zudem wurden Synonyme des oft diskutierten UBIs ersetzt.^[Eine umfassendere, programmatische Ersetzung von weiteren Synonymen wäre sinnvoll, ist hier jedoch nicht erfolgt.]
Nach der Entfernung von Stoppwörtern ("and", "it", etc.) wurden die verbleibenden Wörter mittels des `TreeTagger` Programms wie empfohlen probabilistisch lemmaisiert [@SchmidTreetaggerlanguageindependent1995].


```{r}
r_coms_by_line <- r_coms %>% 
  tidyr::separate_rows(
    body,
    sep = "\n\n"
  ) %>% 
  mutate(body = stringi::stri_replace_all(
    # kind of hacky, but these terms just screw up the results otherwise
    str = body, 
    fixed = c("universal income", "basic income", "universal basic income"), 
    replacement = "ubi", 
    merge = FALSE, 
    vectorize_all = FALSE)
  ) %>% 
  group_by(id) %>% 
  mutate(line = 1:n()) %>% 
  ungroup()
td_coms <- r_coms_by_line %>% 
  unnest_tokens(output = word, input = body, format = "text") %>% 
  anti_join(stop_words) %>% 
  tidyr::separate_rows(  # separate out sepcial charcs
    word,
    sep = "_"
  ) %>% 
  filter(
    is.na(as.numeric(word)),
    str_length(word) > 1
  ) %>% 
  mutate(
    word = stringr::str_extract(word, "[a-z']+")
  )

# lemmatization
if (FALSE) {
  # only run this locally, where treetagger is available
  # TODO would be better to do this with proper dep management apt get and homebrew
  withr::with_dir(
    new = "treetagger/",
    code = processx::run(command = "sh", args = "install-tagger.sh"))
  textstem::make_lemma_dictionary(
  td_coms$word,
  engine = "treetagger",
  lang = "en"
  ) %>% 
    readr::write_rds(path = "dict.rds")
}
dict <- readr::read_rds(path = "dict.rds")

td_coms <- td_coms %>% 
  mutate(
    word = textstem::lemmatize_strings(x = word, dictionary = dict),
    submission_title = stringr::str_trunc(
      string = submission_title,
      width = 15,
      side = "right",
      ellipsis = "..."
    )
  )

# TODO replacing with more common synonyms via WordNet might be even better, http://www.bernhardlearns.com/2017/04/cleaning-words-with-r-stemming.html
```

Es verbleiben über **`r nrow(td_coms)`** Beobachtungen von **`r length(unique(td_coms$word))`** unikalen Wörter, darunter **`r count(x = td_coms, word, sort = TRUE) %>% filter(n > 10) %>% nrow()`** mit mehr als 10 Nennungen.

```{r, fig.cap="Unigram der über 300 Mal genannten Wörter"}
count(x = td_coms, word, sort = TRUE) %>% 
  filter(n > 300) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) + 
  coord_flip()
```

Bereits aus dieser einfachen Zusammenfassung ergibt sich der starke Fokus der Reaktionen auf Automatisierung (`automation`, `robot`, `automated`), das UBI (`ubi`) sowie, zu einem geringeren Grad, grundsätzlichere Konzepte der Gegenwart (`society`, `company`, `government`, `wealth`).
Aus der oberflächlichen Betrachtung einiger Kommentare ergibt sich, dass Automatisierung hier keinesfalls nur, oder überwiegend als Bedrohung betrachtet wird, sondern durchaus als "Bullshit"-befreiende Technologien.
Der Fokus auf technologischen Wandel unterstützt auch eine pragmatische Interpretation der Kritik:
Gemessen an den Möglichkeiten neuer Technologien erscheint *mehr* Arbeit als unsinnig, weil ersetzbar.

```{r, fig.cap="Vergleich der Unigrams in Prozenten der drei größten Threads", fig.width=9, fig.height=9, warning=FALSE}
td_coms %>% 
  group_by(submission_title) %>% 
  filter(n() > 1900) %>% 
  count(submission_title, word) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(submission_title, proportion) %>% 
  gather(submission_title, proportion, 2:6, -"Forget fears...") %>% 
  ggplot(
    mapping = aes(
      x = proportion, 
      y = `Forget fears...`, 
      color = abs(`Forget fears...` - proportion)
    )
  ) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~submission_title, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Forget fears of automation ... (wired.com)", x = NULL)
```

Vergleicht man die fünf größsten Einreichungen, zeigt sich ein etwas nuancierteres Bild.
Im Unterschied zum dominanten, größten Einreichung (auf der y-Achse, basierend auf dem Wired Artikel), stehen bei den Threads "Bullshit Jobs" und "On bullshit" Automatisierung und UBI weniger im Vordergrund.
Bei beiden scheint es eher auf einer Mikroebene um organisationelle und inhaltliche Fragen *innerhalb* eines (Software-)Unternehmens zu gehen (siehe etwa die häufigere Verwendung von `administrative`, `code`, `ceo`, `company`).

Die oben stehende Abbildung zeigt die Suchanfragen skaliert nach den höchsten aus einer Gruppe von anderen neuen und trendigen Berufen.
Im Vergleich wird deutlich, dass es sich bei FGM um ein ausgesprochenes Nischenphänomen handelt.

<!-- TODO -->
Der Vergleich der ausgewählten Threads könnte mit Hilfe einer üblichen $tf-idf$ Transformation noch präziser untersucht werden.

Um die oben stehenden Unigrams zu erweitern, bietet sich auch ein Blick auf ein Netzwerk von Bigrams an.

```{r bigram-network, fig.cap="Common bigrams in the reddit comments"}
coms_bigram_graph <- r_coms_by_line %>% 
  unnest_tokens(bigram, body, token = "ngrams", n = 2) %>% 
  # put each in a column, so we can get rid of stop words
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  filter(
    !word1 %in% c(stop_words$word, "http", "www", "https"),
    !word2 %in% c(stop_words$word, "http", "www", "https"),
  ) %>%
  # duplication from above
  tidyr::separate_rows(  # separate out sepcial charcs
    word1,
    sep = "_"
  ) %>% 
  tidyr::separate_rows(  # separate out sepcial charcs
    word2,
    sep = "_"
  ) %>% 
  filter(
    is.na(suppressWarnings(as.numeric(word1))),
    str_length(word1) > 1,
    is.na(suppressWarnings(as.numeric(word2))),
    str_length(word2) > 1,
  ) %>% 
  mutate(
    word1 = stringr::str_extract(word1, "[a-z']+"),
    word2 = stringr::str_extract(word2, "[a-z']+")
  ) %>%  
  mutate(
    word1 = textstem::lemmatize_strings(x = word1, dictionary = dict),
    word2 = textstem::lemmatize_strings(x = word2, dictionary = dict),
    submission_title = stringr::str_trunc(
      string = submission_title,
      width = 15,
      side = "right",
      ellipsis = "..."
    )
  ) %>% 
  count(word1, word2, sort = TRUE) %>% 
  filter(n > 20) %>% 
  igraph::graph_from_data_frame()

library(ggraph)
set.seed(2018)
ggraph(coms_bigram_graph, layout = "fr") +
  geom_edge_link(
    show.legend = FALSE
  ) +
  geom_node_text(mapping = aes(label = name), vjust = 1, hjust = 1) + 
  theme_void()
```

Oben stehend sind häufig vorkommenden (> 20) Bigramme in den reddit Kommentaren in einem Netzwerk abgebildet.
Stoppwörter und Lemmaisierung sind wie oben beschrieben vorgenommen.
Wie sich zeigt, sind die meisten Bigramm-Paare unverbunden, d.h. sie tauchen nur zusammen häufig auf.
Unter diesen Paaren sind übliche Komposita, wie etwa `social security`.
Einzig um das Lemma `job` entspannt sich ein größerer, verbundener Graph, inklusive der Stichwörter `bullshit`.
Der Graph deutet auch erneut auf die diskursive Verbindung der Bullshit Jobs zum UBI hin.

Schließlich ist es interessant zu 

Einige der sonst üblichen Ansätze für Textanalyse erscheinen für den vorliegenden Korpus wenig geeignet.
Eine Sentiment-Analyse, zumindest eine auf Unigrams basierende, ist für die vorliegenden Daten mit gängigen Lexika wenig aussagekräftig.
Der spezifische Kontext (hier: bullshit jobs) von Wörtern kann mit diesen Methoden nicht abgebildet werden.


# Feelgood Management

## Web Trends

Web Trends zu diesen einschlägigen Begriffen:

```{r terms}
fgm_terms <- tibble::tribble(
  ~term, ~short, ~lang,
  '"chief happiness officer"', "cho", "en-US",
  '"happiness officer"', "ho", "en-US",
  '"feelgood manager"', "fgm", "de-DE",
  '"feel good manager"', "fgm2", "de-DE",
  '"chef du bonheur"', "cdb", "fr-FR",
  '"directeur du bonheur"', "ddb", "fr-FR"
)
knitr::kable(x = fgm_terms, caption = "Relevante Suchbegriffe")
```


## Google Books NGrams

Google Books Daten stehen nur bis 2012 zur Verfügung; davor keine Ergebnisse für `chief happiness officer` oder `feelgood manager`.


## Google Trends

```{r, fig.cap="Google Trends von FGM Begriffen (LOESSS)"}
fgm_gtrends <- gtrendsR::gtrends(
  keyword = fgm_terms$term[1:5], 
  time = "all", 
  low_search_volume = TRUE
)
# country is mostly france
# region and city is paris, nothing much else
ggplot(
  data = fgm_gtrends$interest_over_time,
  mapping = aes(
    x = date,
    y = hits,
    color = keyword
  )
) +
  geom_smooth(formula = y ~ x, method = "loess", se = TRUE, na.rm = TRUE, span = 0.2) +
  ylim(0, 100) +  # because google trends are indexed to 100
  scale_x_date(date_breaks = "1 year") + 
  guides(color = guide_legend(ncol = 2)) + 
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 90, hjust = 1))
```

Die Abbildung gibt die weltweiten Google-Anfragen zu den aufgelisteten Konzepten wieder, indexiert auf einen (rohen) Höchstwert von 100.
Die Rohdaten sind allerdings recht verrausscht, daher wird hier eine erheblich geglättete Form präsentiert, in der der Höchstwert nicht mehr enthalten ist.

Wie man in der o.s. Abbildung sieht, werden die englischen FGM-Begriffe durchgehend ab 2007 verwendet, die deutschen Termini ab etwa 2012, und die französischen erst seit 2016.

Fast alle (berichtete) Suchaktivität entspringt den USA, Deutschland und Frankreich, darunter vor allem die Hauptstädte Berlin und Frankreich.
Schwache Suchaktivitäten sind in den Daten nicht enthalten.


```{r fig.cap="Google Trends von FGM Begriffen im Vergleich (LOESSS)", warning = FALSE}
other_trends <- gtrendsR::gtrends(
  keyword = c("scrum master", "data scientist", "social media manager", "growth hacker"),
  time = "all",
  low_search_volume = TRUE
)

# weirdly, above sometimes includes characters ">1" as hits, so this needs to be fixed
other_trends$interest_over_time$hits <- as.integer(other_trends$interest_over_time$hits)
other_trends$interest_over_time$hits[is.na(other_trends$interest_over_time$hits)] <- 0

df <- dplyr::bind_rows(
  list(
    fgm = fgm_gtrends$interest_over_time, 
    others = other_trends$interest_over_time
  ), 
  .id = "group"
) %>% 
  as_tibble()


# now because these two datasets are separately indexed to 100, we need to download the two highest together, to find the right multiplier
highest <- group_by(df, group) %>% 
  filter(hits == max(hits)) %>% 
  select(keyword) %>% 
  deframe()
multiplier <- gtrendsR::gtrends(
  keyword = highest,
  time = "all",
  low_search_volume = TRUE
) %>% 
  extract2("interest_over_time") %>% 
  group_by(keyword) %>% 
  filter(hits == max(hits)) %>% 
  select(hits) %>% 
  deframe() %>% 
  as.integer() %>% 
  min() %>% 
  divide_by(100)

df <- mutate(
  .data = df,
  hits = if_else(
    condition = group == "fgm",
    true = hits * multiplier,
    false = hits
  )
)

g <- ggplot(
  data = df,
  mapping = aes(
    x = date,
    y = hits,
    color = keyword,
    linetype = group
  )
) +
  geom_smooth(formula = y ~ x, method = "loess", se = TRUE, na.rm = TRUE, span = 0.3) +
  ylim(0, 100) +  # because google trends are indexed to 100
  scale_x_date(date_breaks = "1 year") + 
  guides(color = guide_legend(ncol = 2)) + 
  theme(
    legend.position = "bottom", 
    axis.text.x = element_text(angle = 90, hjust = 1)
  )
g
```

## Twitter Trends

```{r twitter_grab, eval=FALSE, include=FALSE}
# this only works locally, because of keys
tweets <- purrr::imap_dfr(
  .x = list(
    fgm = fgm_terms$term,
    bs = c(
      '"bullshit jobs"',
      '"bullshit job"',
      "#bullshitjob", 
      "#bullshitjobs"
    )
  ),
  .id = "group",
  .f = function(x, y) {
    rtweet::search_tweets2(
      q = x,
      n = 18000,
      type = "mixed",
      include_rts = TRUE,
      parse = TRUE
    )
  }
)
readr::write_rds(x = tweets, path = "tweets.rds")
```


```{r twitter_load}
tweets <- readr::read_rds(path = "tweets.rds")
n_terms <- group_by(.data = tweets, group) %>% 
  summarise(n = n()) %>% 
  deframe()
```

In der vergangenen Woche wurden zu allen o.g. Stichwörtern **`r n_terms["fgm"]`** Tweets (*inklusive retweets!*) abgesetzt.
Für eine tiefergehende Analyse sind zu wenig.
Nach kursorischer Untersuchung handelt es sich überwiegend um Links zu relativ wenigen Artikeln.

Im Vergleich wurde zum Thema Bullshit Jobs **`r n_terms["bs"]`** Mal getweetet.
Hier ist eventuelle eine Sentiment-Analyse o.ä. möglich.

Twitter hat kürzlich die Bedingungen ihrer API geändert; frei verfügbar sind nur noch Daten der letzten 6-9 Tage.


# Bibliografie
